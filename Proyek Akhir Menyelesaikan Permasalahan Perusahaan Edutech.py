# -*- coding: utf-8 -*-
"""Proyek Akhir Menyelesaikan Permasalahan Perusahaan Edutech.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iLBvWoJ8KSDKYKEsCYq1hI5GMJx2R5Od

# Proyek Akhir: Menyelesaikan Permasalahan Perusahaan Edutech

- Nama:Era Syafina
- Email:Erasyafina025@gmail.com
- Id Dicoding:Ersyafin

## Persiapan

### Menyiapkan library yang dibutuhkan

Pada tahap ini, berbagai library diimpor untuk mendukung proses analisis data, mulai dari pra-pemrosesan hingga pembuatan dan evaluasi model. Seluruh library ini berperan penting dalam siklus analisis data, mulai dari pemrosesan awal hingga evaluasi hasil model.
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import xgboost as xgb
import joblib

"""### Menyiapkan data yang akan digunakan

Pada tahap ini, data yang sudah diimpor diperlihatkan dengan menggunakan fungsi data.head() untuk menampilkan beberapa baris pertama sebagai contoh. Hal ini bertujuan agar kita dapat melihat gambaran umum tentang struktur dataset, termasuk nama kolom dan nilai-nilai awal dari setiap kolom.
"""

data = pd.read_csv('https://raw.githubusercontent.com/dicodingacademy/dicoding_dataset/main/students_performance/data.csv', sep=';')

# Menampilkan 5 data teratas
print("Data Sample:")
print(data.head())

"""## Data Understanding

Pada tahap ini, dilakukan eksplorasi awal terhadap dataset untuk mendapatkan pemahaman mengenai struktur data, pola distribusi nilai, serta mengidentifikasi potensi masalah seperti data yang hilang. Langkah-langkah yang dilakukan meliputi:

* **Informasi Data:** Dengan memanfaatkan fungsi `data.info()`, diperoleh informasi tentang jumlah kolom, tipe data, serta jumlah nilai non-null pada tiap kolom. Tahap ini membantu mengenali adanya missing values maupun kebutuhan konversi tipe data.

* **Statistik Deskriptif:** Fungsi `data.describe()` digunakan untuk menampilkan ringkasan statistik seperti rata-rata, median, standar deviasi, serta nilai minimum dan maksimum pada kolom numerik. Statistik ini memberikan gambaran umum terkait pola distribusi data.

* **Pemeriksaan Missing Values:** Fungsi `data.isnull().sum()` dipakai untuk menghitung jumlah nilai yang hilang di setiap kolom. Informasi ini menjadi dasar dalam menentukan strategi penanganan data yang tidak lengkap.

* **Distribusi Variabel Target:** Distribusi kolom target (dalam kasus ini adalah kolom Status) dianalisis menggunakan `value_counts()` untuk mengetahui proporsi masing-masing kategori. Hal ini penting untuk mengevaluasi apakah data tersebut seimbang atau tidak.

* **Visualisasi Distribusi Target:** Dengan menggunakan `sns.countplot()`, dibuat grafik yang menggambarkan distribusi variabel target secara visual. Visualisasi ini memudahkan pemahaman mengenai perbandingan antar kelas secara intuitif.

Keseluruhan langkah ini bertujuan memastikan bahwa data sudah siap untuk diproses lebih lanjut pada tahap preprocessing dan pembuatan model, serta untuk mengidentifikasi potensi masalah yang perlu ditangani terlebih dahulu.
"""

print("\nInformasi Data:")
print(data.info())

print("\nStatistik Data:")
print(data.describe())

print("\nCek jumlah missing values:")
print(data.isnull().sum())

print("\nCek distribusi target (Dropout atau tidak):")
print(data['Status'].value_counts())

sns.countplot(x='Status', data=data)
plt.title('Distribusi Siswa Dropout vs Tidak Dropout')
plt.show()

"""### Exploratory Data Analysis (EDA)"""

# EDA Univariate - Variabel Numerik
import matplotlib.pyplot as plt
import seaborn as sns

numerical_cols = data.select_dtypes(include='number').columns
for col in numerical_cols:
    plt.figure(figsize=(6, 4))
    sns.histplot(data[col], kde=True, bins=30)
    plt.title(f'Distribusi {col}')
    plt.xlabel(col)
    plt.ylabel('Frekuensi')
    plt.tight_layout()
    plt.show()

# EDA Univariate - Variabel Kategorikal
categorical_cols = data.select_dtypes(include='object').columns
for col in categorical_cols:
    plt.figure(figsize=(6, 4))
    sns.countplot(y=col, data=data, order=data[col].value_counts().index)
    plt.title(f'Distribusi Kategori: {col}')
    plt.xlabel('Jumlah')
    plt.ylabel(col)
    plt.tight_layout()
    plt.show()

# Korelasi ulang untuk seluruh fitur numerik
plt.figure(figsize=(15, 10))
sns.heatmap(data[numerical_cols].corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Matriks Korelasi Fitur Numerik')
plt.tight_layout()
plt.show()

# Pairplot sebagian fitur penting (jika tersedia)
selected_cols = ['math score', 'reading score', 'writing score']
if all(col in data.columns for col in selected_cols):
    sns.pairplot(data[selected_cols])
    plt.suptitle('Pairplot Beberapa Fitur Skor', y=1.02)
    plt.show()

# EDA Multivariate - Pairplot untuk beberapa fitur numerik
sns.pairplot(data[numerical_cols])
plt.suptitle('Pairplot Variabel Numerik', y=1.02)
plt.show()

# Boxplot: Hubungan antara skor matematika dan jenis kelamin (contoh multivariat)
if 'math score' in data.columns and 'gender' in data.columns:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x='gender', y='math score', data=data)
    plt.title('Math Score berdasarkan Gender')
    plt.tight_layout()
    plt.show()

"""**Tahap Data Understanding** dilakukan eksplorasi awal untuk memahami struktur dataset, distribusi nilai, dan potensi masalah seperti missing values.

* **Informasi Data (data.info()):**
  Dataset berisi 4424 baris dan 39 kolom, tanpa nilai hilang. Terdiri dari 30 kolom bertipe int64 dan 9 kolom bertipe float64.

* **Statistik Deskriptif (data.describe()):**
  Menampilkan ringkasan statistik kolom numerik, seperti rata-rata, standar deviasi, nilai minimum dan maksimum. Contohnya, kolom Admission\_grade rata-rata 127,3 dengan rentang 0â€“200.

* **Cek Missing Values (data.isnull().sum()):**
  Tidak ditemukan nilai yang hilang di semua kolom.

* **Distribusi Target (data\['Status'].value\_counts()):**
  Menunjukkan jumlah siswa pada kategori Dropout, Enrolled, dan Graduate untuk melihat keseimbangan kelas.

* **Visualisasi Distribusi Target (sns.countplot()):**
  Grafik batang memudahkan visualisasi proporsi tiap kategori target, penting untuk antisipasi bias pada model.

**Kesimpulan:**
Dataset bersih tanpa missing values, beberapa kolom numerik memiliki skala berbeda sehingga perlu normalisasi, dan distribusi target harus diperhatikan terutama jika tidak seimbang.

## Data Preparation / Preprocessing

Pada tahap ini, data dipersiapkan agar optimal untuk pemodelan dengan langkah-langkah berikut:

* **Encoding Target:** Kolom Status dikonversi menjadi numerik, dengan Dropout diberi label 1 dan Enrolled serta Graduate diberi label 0. Kolom Status asli kemudian dihapus.

* **Feature Engineering:** Dibuat fitur baru seperti *avg\_grade* (rata-rata nilai dua semester) dan *approval\_rate* (rasio mata kuliah yang disetujui), serta mempertahankan fitur penting lainnya.

* **Pengecekan dan Penanganan Missing Values:** Semua kolom dicek dan jika ada nilai hilang, diisi dengan 0.

* **Encoding Fitur Kategorikal:** Seluruh fitur kategorikal diubah ke nilai numerik menggunakan LabelEncoder.

* **Pemisahan Data:** Data dibagi menjadi fitur (X) dan target (y).

* **Normalisasi:** Fitur dinormalisasi dengan StandardScaler agar memiliki rata-rata 0 dan deviasi standar 1, guna menghindari bias skala.

* **Pembagian Data Latih dan Uji:** Dataset dibagi 80:20 menggunakan stratifikasi agar proporsi kelas tetap seimbang.

Seluruh proses ini bertujuan memastikan data siap dan berkualitas untuk pemodelan machine learning.
"""

# Encode target (Status)
# Misal: 'Enrolled' = 0, 'Graduate' = 0, 'Dropout' = 1 (Dropout = 1, yang lain 0)
data['Dropout'] = data['Status'].apply(lambda x: 1 if x == 'Dropout' else 0)

# Hapus kolom 'Status' karena sudah diubah
data.drop(columns=['Status'], inplace=True)

# Feature Engineering sederhana
data['avg_grade'] = (data['Curricular_units_1st_sem_grade'] + data['Curricular_units_2nd_sem_grade']) / 2
data['approval_rate'] = (data['Curricular_units_1st_sem_approved'] + data['Curricular_units_2nd_sem_approved']) / \
                        (data['Curricular_units_1st_sem_enrolled'] + data['Curricular_units_2nd_sem_enrolled'] + 1e-5)
data['Age_at_enrollment'] = data['Age_at_enrollment']
data['Debtor'] = data['Debtor']
data['Scholarship_holder'] = data['Scholarship_holder']
data['Tuition_fees_up_to_date'] = data['Tuition_fees_up_to_date']

# Cek missing values
print("\nJumlah missing values per kolom:")
print(data.isnull().sum())

# Mengisi missing value (kalau ada)
data.fillna(0, inplace=True)

# Encode semua fitur kategorikal
categorical_cols = data.select_dtypes(include=['object']).columns
label_encoders = {}

for col in categorical_cols:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    label_encoders[col] = le

# Split features dan target
X = data.drop(columns=['Dropout'])
y = data['Dropout']

# Normalisasi fitur
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split train test
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

"""## Modeling

Pada tahap ini, dilakukan pembangunan model machine learning untuk memprediksi risiko siswa dropout dengan langkah-langkah sebagai berikut:

* **Pembuatan Model Awal:**
  Menggunakan algoritma XGBClassifier dari XGBoost karena kemampuannya menangani data berdimensi tinggi dan performa unggul pada klasifikasi biner. Parameter dasar meliputi `objective='binary:logistic'`, `eval_metric='logloss'`, `use_label_encoder=False`, dan `random_state=42`.

* **Penyetelan Hyperparameter:**
  GridSearchCV digunakan untuk mencari kombinasi parameter terbaik dengan opsi seperti `max_depth` (3,5,7), `learning_rate` (0.01,0.1,0.2), `n_estimators` (100,200), `subsample` (0.8,1), dan `colsample_bytree` (0.8,1). Cross-validation 3-fold dilakukan untuk menghindari overfitting.

* **Pelatihan Model:**
  Model dilatih pada data training dengan berbagai kombinasi hyperparameter yang diuji.

* **Pemilihan Model Terbaik:**
  Model dengan performa terbaik dari hasil validasi dipilih dan disimpan menggunakan joblib untuk penggunaan ulang tanpa pelatihan ulang.

Tujuan keseluruhan adalah menghasilkan model yang akurat dan andal dalam memprediksi dropout siswa.
"""

# Membuat model awal
model = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    use_label_encoder=False,
    random_state=42
)

# Hyperparameter tuning sederhana
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [100, 200],
    'subsample': [0.8, 1],
    'colsample_bytree': [0.8, 1]
}

grid_search = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    cv=3,
    n_jobs=-1,
    verbose=1
)

# Training
grid_search.fit(X_train, y_train)

# Best model
best_model = grid_search.best_estimator_

# Simpan model
joblib.dump(best_model, 'model_dropout_xgboost.pkl')

# Simpan scaler dan label encoder
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(label_encoders, 'label_encoders.pkl')

print("\nModel, Scaler, dan Label Encoders berhasil disimpan!")

"""## Evaluation

Pada tahap evaluasi, model diuji menggunakan data pengujian (X\_test dan y\_test) untuk menilai kemampuannya dalam memprediksi dropout siswa. Langkah-langkah evaluasi meliputi:

* **Prediksi Data Uji:**
  Model terbaik digunakan untuk memprediksi label kelas dengan `predict()` dan probabilitas prediksi dengan `predict_proba()`.

* **Laporan Klasifikasi:**
  Menggunakan `classification_report` untuk melihat metrik penting seperti precision, recall, F1-score, dan support per kelas, guna mengevaluasi kemampuan model membedakan siswa dropout dan non-dropout.

* **Matriks Kebingungan:**
  `confusion_matrix` menunjukkan jumlah prediksi benar dan salah, membantu mengidentifikasi jenis kesalahan klasifikasi yang terjadi.

* **Akurasi Model:**
  `accuracy_score` menghitung proporsi prediksi tepat dari total data uji, memberikan gambaran umum efektivitas model.

Evaluasi ini membantu menentukan seberapa baik model dalam memprediksi dropout serta kebutuhan perbaikan untuk meningkatkan performa.
"""

# Predict
y_pred = best_model.predict(X_test)

# Evaluasi
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

accuracy = accuracy_score(y_test, y_pred)
print(f"\nAkurasi Model setelah tuning: {accuracy}")

"""**Laporan Klasifikasi:**

* **Precision:**

  * Kelas 0 (tidak dropout) mencapai 0.89, artinya 89% prediksi tidak dropout tepat.
  * Kelas 1 (dropout) sebesar 0.86, menunjukkan 86% prediksi dropout akurat.
* **Recall:**

  * Kelas 0 memiliki recall 0.94, model berhasil mendeteksi 94% siswa yang benar-benar tidak dropout.
  * Kelas 1 recall-nya 0.76, hanya 76% siswa dropout teridentifikasi.
* **F1-Score:**

  * Kelas 0 sebesar 0.92, menunjukkan keseimbangan baik antara precision dan recall.
  * Kelas 1 sebesar 0.81, performa cukup baik namun masih bisa diperbaiki.
* **Akurasi:**
  Model mencapai akurasi keseluruhan 88%, artinya prediksi tepat pada 88% data uji.

**Matriks Kebingungan:**

* TP (dropout terdeteksi benar): 216
* TN (tidak dropout terdeteksi benar): 567
* FP (salah klasifikasi tidak dropout sebagai dropout): 34
* FN (dropout yang tidak terdeteksi): 68

**Kesimpulan:**
Model bekerja cukup baik dengan akurasi 88%, lebih handal dalam mengenali siswa tidak dropout dibandingkan siswa dropout. Namun, deteksi siswa dropout (kelas 1) masih bisa ditingkatkan, khususnya recall-nya agar lebih banyak kasus dropout teridentifikasi.

**Rekomendasi:**
Untuk meningkatkan deteksi dropout, dapat digunakan teknik penyeimbangan data seperti SMOTE atau menyesuaikan threshold probabilitas prediksi.

## Mengupload dataframe yang sudah bersih ke supabase
"""

from sqlalchemy import create_engine

URL = "postgresql://postgres.blgvdilmdzkcggxuhlyo:Rafinaaa14_$@aws-0-ap-southeast-1.pooler.supabase.com:6543/postgres"

engine = create_engine(URL)
data.to_sql('dataset', engine)

!pip freeze > requirements.txt